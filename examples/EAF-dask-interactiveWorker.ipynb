{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa111dea-cbf0-417b-a00b-69c254ecb768",
   "metadata": {},
   "source": [
    "## Basic cluster creation\n",
    "* The following code will connect to the Dask Gateway server, use jupyterhub OAuth to authenticate and ask the server for a new cluster.\n",
    "* This willl trigger the server to start a Scheduler pod in OKD, which will be exposed via Traefik. \n",
    "* To connect to the scheduler, use the public route: tls://dask-gateway-tls.fnal.gov:443\n",
    "* This route is static for now. Functionality is being built to expose it dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc15031-ac4a-43d8-b990-0d35b86a9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "import time\n",
    "import socket\n",
    "from lpcdaskgateway import LPCGateway\n",
    "\n",
    "gateway = LPCGateway()\n",
    "cluster = gateway.new_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3e250-7ca5-4eab-8392-95ed87ec17bf",
   "metadata": {},
   "source": [
    "## Staging and interactive worker launching\n",
    "* In order to have hybrid Dask clusters, we have added a mode of operation to Dask Gateway Server in which external Dask Singularity workers join a containerized Kubernetes scheduler on the fly. \n",
    "* The ideal and final goal is to submit these into the grid as batch jobs. This is an intermediate step towards that.\n",
    "* The following code will use the Dask Gateway API to interact with the cluster, obtain information for the worker to properly connect and authenticate, and finally join the scheduler in a 1:1 hybrid cluster.\n",
    "* To connect to the scheduler, use the public route: tls://dask-gateway-tls.fnal.gov:443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90654249-cda6-4402-ae8e-6ce1b441dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = cluster.name\n",
    "print(\"Staging a static worker for cluster: \"+cluster_name)\n",
    "\n",
    "import pwd\n",
    "\n",
    "def prepareWorkerArea(cluster):\n",
    "    username = pwd.getpwuid( os.getuid() )[ 0 ]\n",
    "    security = cluster.security\n",
    "    tmproot = f\"/uscmst1b_scratch/lpc1/3DayLifetime/{username}/{cluster_name}\"\n",
    "    credentials_dir = f\"{tmproot}/dask-credentials\"\n",
    "    worker_space_dir = f\"{tmproot}/dask-worker-space\"\n",
    "    image_name = f\"/cvmfs/unpacked.cern.ch/registry.hub.docker.com/coffeateam/coffea-dask-cc7-gateway:0.7.12-fastjet-3.3.4.0rc9-g8a990fa\"\n",
    "    worker_name = \"dask-gateway-localworker-\"+socket.gethostname()\n",
    "    print(\"-- Creating base directory and copying credentials\")\n",
    "    os.makedirs(tmproot, exist_ok=True)\n",
    "    os.makedirs(credentials_dir, exist_ok=True)\n",
    "    os.makedirs(worker_space_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{credentials_dir}/dask.crt\", 'w') as f:\n",
    "        f.write(security.tls_cert)\n",
    "    with open(f\"{credentials_dir}/dask.pem\", 'w') as f:\n",
    "        f.write(security.tls_key)\n",
    "    with open(f\"{credentials_dir}/api-token\", 'w') as f:\n",
    "        f.write(os.environ['JUPYTERHUB_API_TOKEN'])\n",
    "        \n",
    "    # Prepare worker.env\n",
    "    print(\"-- Writing worker environment file (not supported on batch workers)\")\n",
    "    env = \"\"\"DASK_GATEWAY_WORKER_NAME=\"\"\"+worker_name+\"\"\"\n",
    "DASK_GATEWAY_API_URL=\"https://dask-gateway-api.fnal.gov/api\"\n",
    "DASK_GATEWAY_CLUSTER_NAME=\"\"\"+cluster_name+\"\"\"\n",
    "DASK_GATEWAY_API_TOKEN=/etc/dask-credentials/api-token\n",
    "DASK_DISTRIBUTED__COMM__TLS__CA_FILE=/etc/dask-credentials/dask.crt\n",
    "DASK_DISTRIBUTED__COMM__TLS__SCHEDULER__CERT=/etc/dask-credentials/dask.crt\n",
    "DASK_DISTRIBUTED__COMM__TLS__SCHEDULER__KEY=/etc/dask-credentials/dask.pem\n",
    "DASK_DISTRIBUTED__COMM__TLS__WORKER__CERT=/etc/dask-credentials/dask.crt\n",
    "DASK_DISTRIBUTED__COMM__TLS__WORKER__KEY=/etc/dask-credentials/dask.pem\n",
    "DASK_DISTRIBUTED__LOGGING__DISTRIBUTED=debug\"\"\"\n",
    "    \n",
    "    with open(f\"{tmproot}/worker.env\", 'w+') as f:\n",
    "        f.writelines(env)\n",
    "        \n",
    "    # Prepare singularity command\n",
    "    print(\"-- Writing singularity exec command\")\n",
    "    sing = \"\"\"#!/bin/bash\n",
    "singularity exec --env-file worker.env  --bind ${PWD}/dask-worker-space:/srv/dask-worker-space  --bind ${PWD}/dask-credentials:/etc/dask-credentials /cvmfs/unpacked.cern.ch/registry.hub.docker.com/coffeateam/coffea-dask-cc7-gateway:latest dask-worker --name \"\"\"+worker_name+\"\"\" --worker-port 10000:10070 --no-nanny --no-dashboard --local-directory /srv --nthreads 2 --nprocs 1 tls://dask-gateway-tls.fnal.gov:443\"\"\"\n",
    "    \n",
    "    with open(f\"{tmproot}/start.sh\", 'w+') as f:\n",
    "        f.writelines(sing)\n",
    "    os.chmod(f\"{tmproot}/start.sh\", 0o775)\n",
    "    return(tmproot)\n",
    "        \n",
    "tmproot = prepareWorkerArea(cluster)\n",
    "print(\"Done\")\n",
    "print(\"Attention: Manual steps ahead!\")\n",
    "print(\"-- Login to a cmslpc interactive node capable of running Singularity\")\n",
    "print(\"-- `cd` to \"+tmproot)\n",
    "print(\"-- and finally, run the `start.sh` script\")\n",
    "print(\"The script will start a Singularity container running a remote dask-gateway worker,\")\n",
    "print(\"which will join your scheduler and be ready to run Dask analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28830fc1-815d-4fc6-a6dc-972953fd64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e031d-c104-46b4-a368-daebeee7f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cluster))\n",
    "print(cluster.scheduler_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccf81d-533a-4eba-a64a-3fdc07fe50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a client for connecting to your cluster scheduler\n",
    "# Your cluster should be ready to take requests\n",
    "#cluster = gateway.connect(cluster_name)\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e56f7d-970e-4218-b2f4-2ed57c75c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computations on the cluster\n",
    "# At this point you should be able to use normal dask methods to do work.\n",
    "# For example, here we take the mean of a random array.\n",
    "import dask.array as da\n",
    "a = da.random.normal(size=(1000, 1000), chunks=(500, 500))\n",
    "a.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01837e8b-eaba-4361-9de2-2eabf0fc637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell to remove/shutdown the cluster\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d4b1f-30a9-47d7-8df8-145d351c1f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
